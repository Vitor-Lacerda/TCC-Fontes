function [y1] = NNGerada(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 07-Jul-2016 17:24:42.
% 
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 5xQ matrix, input #1
% and returns:
%   y = 2xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

  % ===== NEURAL NETWORK CONSTANTS =====
  
  % Input 1
  x1_step1_xoffset = [39;88;132;69;75.7004310344828];
  x1_step1_gain = [0.0119760479041916;0.0119760479041916;0.000513478818998716;0.0108695652173913;0.0917737365492234];
  x1_step1_ymin = -1;
  
  % Layer 1
  b1 = [2.0207748380025201;-1.810760636159779;-1.2967477261690621;0.94953497808988063;0.20950374062567217;-1.6783081333227525;0.12819268237538861;2.7060909244720257;-2.0003430931131279;2.0451893186612189];
  IW1_1 = [-1.8896117382062485 -1.9013124623854603 -0.072812770581986694 0.028494787657338136 -0.76427175043916251;1.406805300419993 -1.0537902047432517 1.1953071338982022 0.3611412764264873 0.012797574865062257;1.1900059584749372 -1.2822632695411373 0.87288933722940576 -0.90043458181185854 0.65279950299922107;0.67736330784984788 -1.0023342461113811 -1.0588362035549732 -1.8454045238099024 0.93144652976363551;0.84197050417818486 1.0778638047257967 0.09032671899369546 0.52510203007644785 1.9370995884546061;0.57207433980875622 0.38069126562463873 0.30095334788198935 2.3837650795062584 0.33663351429423416;0.040266428390770076 -0.31309499616431863 2.5336663959444299 -0.65552368544009609 -1.7793601731518727;0.049796382813243417 1.0825961219647804 4.0312957597825392 -1.4960565853576175 -0.41497079068476761;-1.0409306207475792 -1.4597493802906971 0.15634831292796056 -0.54102898249225184 -1.0836082718215045;0.86051670378280276 -0.26926314806941509 -1.0380456819264812 1.1058325765679202 1.4412973141009739];
  
  % Layer 2
  b2 = [-0.87993923959717713;0.049434582948242156];
  LW2_1 = [-1.0776563273924631 -0.48212878231269746 -1.2534671485056541 -0.56888688899113526 -1.0568782599372124 1.7825127595330248 -0.95256694036332545 -3.3446588300299513 0.55010444223509569 0.25667956009880294;1.2147903374730664 0.8277059661372197 0.34662518133777537 0.303143957236502 0.70677497186867488 -0.95032732143503362 1.8864014846053001 3.430737325892196 -0.28175455972729857 -0.96932769952232822];
  
  % ===== SIMULATION ========
  
  % Dimensions
  Q = size(x1,2); % samples
  
  % Input 1
  xp1 = mapminmax_apply(x1,x1_step1_gain,x1_step1_xoffset,x1_step1_ymin);
  
  % Layer 1
  a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);
  
  % Layer 2
  a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);
  
  % Output 1
  y1 = a2;
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings_gain,settings_xoffset,settings_ymin)
  y = bsxfun(@minus,x,settings_xoffset);
  y = bsxfun(@times,y,settings_gain);
  y = bsxfun(@plus,y,settings_ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n)
  nmax = max(n,[],1);
  n = bsxfun(@minus,n,nmax);
  numer = exp(n);
  denom = sum(numer,1); 
  denom(denom == 0) = 1;
  a = bsxfun(@rdivide,numer,denom);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
  a = 2 ./ (1 + exp(-2*n)) - 1;
end
